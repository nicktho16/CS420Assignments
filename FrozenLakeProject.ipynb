{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicktho16/CS420Assignments/blob/main/FrozenLakeProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import gym\n",
        "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
        "import numpy as np\n",
        "\n",
        "# Suppress all DeprecationWarnings globally\n",
        "#You can safely ignore warnings. This code helps supress some\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n"
      ],
      "metadata": {
        "id": "iFgjH2qYii8N"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom 8x8 map\n",
        "custom_map = [\n",
        "    \"SFFFFHFF\",\n",
        "    \"FFFFFFFF\",\n",
        "    \"FFFFFHFF\",\n",
        "    \"FFFFFFFF\",\n",
        "    \"HHHFFHHH\",  # Fifth row: HH FFF HHH\n",
        "    \"FFFFFFFF\",\n",
        "    \"FHFFFFFF\",\n",
        "    \"FHFFFFFG\"\n",
        "]\n",
        "\n",
        "# Create the environment with the custom map\n",
        "env = gym.make('FrozenLake-v1', desc=custom_map, is_slippery=True, new_step_api=True)\n",
        "\n",
        "# Print the number of states and actions\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "24WGVVzwd_GJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9\n",
        "theta = 1e-6\n",
        "V = np.zeros(n_states)\n",
        "policy = np.zeros(n_states, dtype=int)"
      ],
      "metadata": {
        "id": "eekCsO5ieR4o"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The discount factor (gamma) determines the importance of future rewards compared to immediate rewards. As gamma gets higher, the agent values future rewards almost as much as immediate rewards, so the agent will be more inclined to explore the grid and plan for long-term gains. In this case with gamma = 0.9, this indicates that the agent gives significant weight to future rewards but not as much as he does to immediate rewards since gamma doesnt equal 1."
      ],
      "metadata": {
        "id": "v6a8YN3q4HJV"
      }
    },
    {
      "source": [
        "def value_iteration():\n",
        "    global V, policy\n",
        "    while True:\n",
        "        delta = 0\n",
        "        new_V = np.copy(V)\n",
        "        for s in range(n_states):\n",
        "            q_values = np.zeros(n_actions)\n",
        "            for a in range(n_actions):\n",
        "                for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n",
        "                    if done:\n",
        "                        q_values[a] += prob * reward\n",
        "                    else:\n",
        "                        q_values[a] += prob * (reward + gamma * V[next_state])\n",
        "            new_V[s] = max(q_values)\n",
        "            delta = max(delta, abs(V[s] - new_V[s]))\n",
        "            policy[s] = np.argmax(q_values)\n",
        "        V = new_V\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "def extract_policy():\n",
        "    for s in range(n_states):\n",
        "        q_values = np.zeros(n_actions)\n",
        "        for a in range(n_actions):\n",
        "            for prob, next_state, reward, done in env.unwrapped.P[s][a]:\n",
        "                if done:\n",
        "                    q_values[a] += prob * reward\n",
        "                else:\n",
        "                    q_values[a] += prob * (reward + gamma * V[next_state])\n",
        "        policy[s] = np.argmax(q_values)\n",
        "\n",
        "def display_policy():\n",
        "    actions = ['←', '↓', '→', '↑']\n",
        "    grid_size = int(np.sqrt(n_states))\n",
        "\n",
        "    lake_map = env.unwrapped.desc\n",
        "\n",
        "    print(\"\\nOptimal Policy:\\n\")\n",
        "    for i in range(grid_size):\n",
        "        row = \"\"\n",
        "        for j in range(grid_size):\n",
        "            state = i * grid_size + j\n",
        "            tile = lake_map[i, j].decode(\"utf-8\")\n",
        "\n",
        "            if tile == 'H':\n",
        "                cell = 'H'  # Hole\n",
        "            elif tile == 'G':\n",
        "                cell = 'G'  # Goal\n",
        "            elif tile == 'S':\n",
        "                cell = 'S'  # Start position\n",
        "            else:\n",
        "                cell = actions[policy[state]]\n",
        "            row += f\"| {cell} \"\n",
        "        row += \"|\"\n",
        "        print(row)\n",
        "        print(\"-\" * (grid_size * 4 + 1))\n",
        "\n",
        "value_iteration()\n",
        "extract_policy()\n",
        "display_policy()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcaeee67-0742-4dec-8809-70f50083b960",
        "id": "ScAyZ_RZTUv2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal Policy:\n",
            "\n",
            "| S | → | → | → | ← | H | → | ← |\n",
            "---------------------------------\n",
            "| ↓ | → | → | ↓ | ← | ↓ | ↓ | ↓ |\n",
            "---------------------------------\n",
            "| ↓ | → | → | ↓ | ← | H | → | ↓ |\n",
            "---------------------------------\n",
            "| ↑ | ↑ | ↑ | → | ← | ↓ | ↑ | ↑ |\n",
            "---------------------------------\n",
            "| H | H | H | → | ← | H | H | H |\n",
            "---------------------------------\n",
            "| ↓ | ↓ | ↓ | → | ↓ | ↓ | ↓ | ↓ |\n",
            "---------------------------------\n",
            "| ← | H | → | → | → | → | ↓ | ↓ |\n",
            "---------------------------------\n",
            "| ← | H | → | → | → | → | → | G |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The state-value function is represented here by V, which is the variable that stores the value of each state. It represents the expected discounted reward when starting from state s and following the optimal policy pi. It is iteratively updated using the Bellman Update Equation in the value_iteration() function until V is smaller than theta."
      ],
      "metadata": {
        "id": "fMPvFsw3-W_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bellman Equation occurs in this code block, with the value_iteration() function"
      ],
      "metadata": {
        "id": "IJmEaK9D7YHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run multiple episodes and compute average reward and steps\n",
        "def run_experiment(policy, env, num_episodes=1000):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    for _ in range(num_episodes):\n",
        "        # Reset the environment (new API returns observation and info)\n",
        "        # The updated reset() method returns observation only\n",
        "        obs = env.reset()\n",
        "        total_reward = 0\n",
        "        step_count = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Choose the action from the computed policy\n",
        "            action = policy[obs]\n",
        "            # Perform the action; new API returns (obs, reward, terminated, truncated, info)\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            step_count += 1\n",
        "        rewards.append(total_reward)\n",
        "        steps.append(step_count)\n",
        "\n",
        "    avg_reward = np.mean(rewards)\n",
        "    avg_steps = np.mean(steps)\n",
        "    print(f\"\\nAfter {num_episodes} episodes:\")\n",
        "    print(f\"Average reward: {avg_reward}\")\n",
        "    print(f\"Average number of steps per episode: {avg_steps}\")\n",
        "    return avg_reward, avg_steps\n"
      ],
      "metadata": {
        "id": "yP8whQ3Wfn5w"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block simulates many different experiments of the frozen lake and collects data on how well the agent performs by taking their average rewards and steps, then it returns these stats which truly evaluates the agents performance."
      ],
      "metadata": {
        "id": "gRuNlwC_LzhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the experiment\n",
        "run_experiment(policy, env, num_episodes=1000)\n",
        "\n",
        "# Reset the environment\n",
        "env.reset()\n",
        "\n",
        "returns = []"
      ],
      "metadata": {
        "id": "ew8xNDQDLdMK",
        "outputId": "5b011153-0ae9-46d8-810b-6b38f8b86a31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After 1000 episodes:\n",
            "Average reward: 0.79\n",
            "Average number of steps per episode: 64.836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZTHYw97MI2Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion Block -"
      ],
      "metadata": {
        "id": "HlNv4FVMOLie"
      }
    }
  ]
}